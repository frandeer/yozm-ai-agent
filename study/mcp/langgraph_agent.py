"""
ì‹¬í”Œí•œ LangGraph + MCP ì—ì´ì „íŠ¸ ì˜ˆì œ
- ìƒíƒœ: ê°„ë‹¨í•œ ë©”ì‹œì§€ì™€ ì‘ë‹µë§Œ ì €ì¥
- ë…¸ë“œ: LLM í˜¸ì¶œ, ë„êµ¬ ì‹¤í–‰, ì‘ë‹µ ìƒì„±
- ì—£ì§€: ì¡°ê±´ë¶€ ë¼ìš°íŒ… (ë„êµ¬ í•„ìš” ì—¬ë¶€)
"""
import asyncio
import os
from typing import Literal
from pydantic import BaseModel, Field

from langchain_openai import ChatOpenAI
from langchain_mcp_adapters.tools import load_mcp_tools
from langchain_core.messages import HumanMessage, AIMessage, ToolMessage
from langgraph.graph import StateGraph, START, END
from mcp import ClientSession
from mcp.client.streamable_http import streamablehttp_client

from utils.logging import langsmith

langsmith("langgraph-ex17-simple")

# .env íŒŒì¼ ì§€ì›
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    pass


# ============================================================
# 1. ìƒíƒœ ì •ì˜ (ì‹¬í”Œ!)
# ============================================================
class AgentState(BaseModel):
    """ì—ì´ì „íŠ¸ ìƒíƒœ"""
    user_message: str = ""
    ai_message: AIMessage | None = None
    tool_results: list[ToolMessage] = Field(default_factory=list)
    final_response: str = ""


# ============================================================
# 2. ì „ì—­ ë³€ìˆ˜
# ============================================================
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
tools_cache = None  # ì „ì—­ ë³€ìˆ˜ë¡œ ë„êµ¬ ìºì‹±


# ============================================================
# 3. ë…¸ë“œ í•¨ìˆ˜ë“¤ (ì‹¬í”Œ!)
# ============================================================


def llm_node(state: AgentState) -> dict:
    """LLMì´ ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ë³´ê³  ë„êµ¬ ì‚¬ìš© ì—¬ë¶€ë¥¼ ê²°ì •"""
    global tools_cache
    
    llm_with_tools = llm.bind_tools(tools_cache)
    
    # ì‹œìŠ¤í…œ ë©”ì‹œì§€ + ì‚¬ìš©ì ë©”ì‹œì§€
    messages = [
        HumanMessage(content=f"""ë‹¹ì‹ ì€ ì¹œì ˆí•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

ì‚¬ìš©ì ì§ˆë¬¸: {state.user_message}

ê°„ë‹¨í•œ ì¸ì‚¬ëŠ” ë„êµ¬ ì—†ì´ ë°”ë¡œ ë‹µë³€í•˜ì„¸ìš”.
ë„êµ¬ê°€ í•„ìš”í•œ ê²½ìš°ì—ë§Œ ì‚¬ìš©í•˜ì„¸ìš”.""")
    ]
    
    ai_msg = llm_with_tools.invoke(messages)
    
    # ë„êµ¬ ì‚¬ìš© ì—¬ë¶€ ë¡œê¹…
    if ai_msg.tool_calls:
        tool_names = [tc["name"] for tc in ai_msg.tool_calls]
        print(f"  ğŸ”§ ë„êµ¬ í˜¸ì¶œ: {', '.join(tool_names)}")
    
    return {"ai_message": ai_msg}


async def tool_node(state: AgentState) -> dict:
    """ë„êµ¬ë¥¼ ì‹¤í–‰ (ë¹„ë™ê¸°) - ëª¨ë“  tool_calls ì²˜ë¦¬"""
    ai_msg = state.ai_message
    
    if not ai_msg.tool_calls:
        return {"tool_results": []}
    
    tool_messages = []
    
    # ëª¨ë“  ë„êµ¬ í˜¸ì¶œ ì²˜ë¦¬
    for tool_call in ai_msg.tool_calls:
        tool_name = tool_call["name"]
        tool_args = tool_call["args"]
        tool_call_id = tool_call["id"]
        
        print(f"  âš™ï¸  ë„êµ¬ ì‹¤í–‰ ì¤‘: {tool_name}({tool_args})")
        
        # ë„êµ¬ ì‹¤í–‰ (ë¹„ë™ê¸°)
        result = None
        for tool in tools_cache:
            if tool.name == tool_name:
                result = await tool.ainvoke(tool_args)
                print(f"  âœ… ë„êµ¬ ê²°ê³¼: {result[:100]}..." if len(str(result)) > 100 else f"  âœ… ë„êµ¬ ê²°ê³¼: {result}")
                break
        
        if result is None:
            result = f"ë„êµ¬ '{tool_name}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            print(f"  âŒ {result}")
        
        # ToolMessage ìƒì„±
        tool_messages.append(
            ToolMessage(content=str(result), tool_call_id=tool_call_id)
        )
    
    return {"tool_results": tool_messages}


def response_node(state: AgentState) -> dict:
    """ìµœì¢… ì‘ë‹µ ìƒì„±"""
    # ë„êµ¬ ì‚¬ìš© ì•ˆ í–ˆìœ¼ë©´ AI ë©”ì‹œì§€ ê·¸ëŒ€ë¡œ ë°˜í™˜
    if not state.ai_message.tool_calls:
        return {"final_response": state.ai_message.content}
    
    # ë„êµ¬ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì¢… ì‘ë‹µ ìƒì„±
    messages = [
        HumanMessage(content=state.user_message),
        state.ai_message,
        *state.tool_results  # ëª¨ë“  ToolMessage ì¶”ê°€
    ]
    
    final_ai_msg = llm.invoke(messages)
    return {"final_response": final_ai_msg.content}


# ============================================================
# 4. ë¼ìš°íŒ… í•¨ìˆ˜ (ì‹¬í”Œ!)
# ============================================================
def should_use_tool(state: AgentState) -> Literal["use_tool", "respond"]:
    """ë„êµ¬ ì‚¬ìš© ì—¬ë¶€ ê²°ì •"""
    if state.ai_message and state.ai_message.tool_calls:
        return "use_tool"
    return "respond"


# ============================================================
# 5. ê·¸ë˜í”„ ìƒì„± (ì‹¬í”Œ!)
# ============================================================
def create_simple_graph():
    """ì‹¬í”Œí•œ LangGraph ìƒì„±"""
    workflow = StateGraph(AgentState)
    
    # ë…¸ë“œ ì¶”ê°€
    workflow.add_node("llm", llm_node)
    workflow.add_node("tool", tool_node)
    workflow.add_node("respond", response_node)
    
    # ì—£ì§€ ì¶”ê°€
    workflow.add_edge(START, "llm")
    workflow.add_conditional_edges(
        "llm",
        should_use_tool,
        {
            "use_tool": "tool",
            "respond": "respond"
        }
    )
    workflow.add_edge("tool", "respond")
    workflow.add_edge("respond", END)
    
    return workflow.compile()


# ============================================================
# 6. ë©”ì¸ ì‹¤í–‰
# ============================================================
async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    global tools_cache
    
    # í™˜ê²½ ì²´í¬
    if not os.getenv("OPENAI_API_KEY"):
        print("âŒ OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        return
    
    server_url = "http://localhost:8000/mcp/"
    
    try:
        print("ğŸ”§ MCP ì„œë²„ ì—°ê²° ì¤‘...\n")
        
        # MCP ì„¸ì…˜ì„ ì „ì²´ ì‹¤í–‰ ê¸°ê°„ ë™ì•ˆ ìœ ì§€
        async with streamablehttp_client(server_url) as (read, write, _):
            async with ClientSession(read, write) as session:
                # 1. ì„¸ì…˜ ì´ˆê¸°í™” ë° ë„êµ¬ ë¡œë“œ
                await session.initialize()
                tools_cache = await load_mcp_tools(session)
                print(f"âœ… ë¡œë“œëœ ë„êµ¬: {[tool.name for tool in tools_cache]}\n")
                
                # 2. ê·¸ë˜í”„ ìƒì„±
                graph = create_simple_graph()
                
                # 3. í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ë“¤
                queries = [
                    # "ì•ˆë…•! ë‚´ ì´ë¦„ì€ ì² ìˆ˜ì•¼",
                    # "ì½”ë“œ ë¦¬ë·°ìš© í”„ë¡¬í”„íŠ¸ë¥¼ ì•Œë ¤ì¤˜",
                    "summarize í”„ë¡¬í”„íŠ¸ë„ ë³´ì—¬ì¤˜",
                ]
                
                # 4. ê° ì§ˆë¬¸ ì²˜ë¦¬ (ì„¸ì…˜ì´ ì‚´ì•„ìˆëŠ” ë™ì•ˆ)
                for i, query in enumerate(queries, 1):
                    print(f"[ì§ˆë¬¸ {i}] {query}")
                    
                    result = await graph.ainvoke({"user_message": query})
                    
                    print(f"[ì‘ë‹µ] {result['final_response']}\n")
                    print("-" * 60 + "\n")
    
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(main())
